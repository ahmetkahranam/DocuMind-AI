import os
import json
import numpy as np
from sentence_transformers import SentenceTransformer

# File reading utility with encoding safety
def safe_read_file(file_path):
    """Safely read file content with proper encoding detection"""
    try:
        filename = os.path.basename(file_path)
        file_extension = os.path.splitext(filename)[1].lower()
        
        print(f"[+] Reading file: {filename} (extension: {file_extension})")
        
        # Handle PDF files
        if file_extension == '.pdf':
            try:
                import PyPDF2
                with open(file_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                print(f"[+] PDF extracted: {len(text)} characters")
                return text
            except ImportError:
                print("[!] PyPDF2 not installed, treating as text")
            except Exception as e:
                print(f"[!] PDF extraction failed: {e}")
        
        # Handle Word documents
        elif file_extension in ['.doc', '.docx']:
            try:
                from docx import Document
                doc = Document(file_path)
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                print(f"[+] Word document extracted: {len(text)} characters")
                return text
            except ImportError:
                print("[!] python-docx not installed, treating as text")
            except Exception as e:
                print(f"[!] Word extraction failed: {e}")
        
        # Handle text files with encoding detection
        content = None
        encodings_to_try = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252', 'iso-8859-1', 'utf-16']
        
        for encoding in encodings_to_try:
            try:
                with open(file_path, 'r', encoding=encoding) as file:
                    content = file.read()
                print(f"[+] File read successfully with {encoding} encoding")
                break
            except UnicodeDecodeError as e:
                print(f"[!] Failed with {encoding}: {e}")
                continue
            except Exception as e:
                print(f"[!] Error with {encoding}: {e}")
                continue
        
        # If all encodings failed, try binary with error handling
        if content is None:
            try:
                with open(file_path, 'rb') as file:
                    binary_data = file.read()
                content = binary_data.decode('utf-8', errors='replace')
                print(f"[+] Binary read with error replacement: {len(content)} characters")
            except Exception as e:
                print(f"[!] Binary read failed: {e}")
                return None
        
        return content
        
    except Exception as e:
        print(f"[!] Critical error reading {file_path}: {e}")
        return None

import json
import logging
import time
import hashlib
from typing import List, Dict, Any, Optional, Union
import numpy as np
from pathlib import Path
import pickle
from datetime import datetime, timedelta
import threading
from multiprocessing import cpu_count

import requests
from tqdm import tqdm
from config import config

logger = logging.getLogger(__name__)


class EmbeddingCache:
    """Embedding cache sistemi"""

    def __init__(self, cache_dir: str = "./embedding_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        self.cache_file = os.path.join(cache_dir, "embedding_cache.pkl")
        self.cache = self._load_cache()

    def _load_cache(self) -> Dict[str, np.ndarray]:
        """Cache'i yÃ¼kle"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, "rb") as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"Cache yÃ¼klenemedi: {e}")
        return {}

    def _save_cache(self):
        """Cache'i kaydet"""
        try:
            with open(self.cache_file, "wb") as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            logger.error(f"Cache kaydedilemedi: {e}")

    def get_hash(self, text: str, model_name: str) -> str:
        """Text + model iÃ§in hash oluÅŸtur"""
        combined = f"{model_name}:{text}"
        return hashlib.md5(combined.encode()).hexdigest()

    def get(self, text: str, model_name: str) -> Optional[np.ndarray]:
        """Cache'den embedding al"""
        hash_key = self.get_hash(text, model_name)
        return self.cache.get(hash_key)

    def set(self, text: str, model_name: str, embedding: np.ndarray):
        """Cache'e embedding ekle"""
        hash_key = self.get_hash(text, model_name)
        self.cache[hash_key] = embedding

        # Periyodik kaydetme (her 100 yeni embedding'de)
        if len(self.cache) % 100 == 0:
            self._save_cache()

    def clear(self):
        """Cache'i temizle"""
        self.cache.clear()
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)

    def get_stats(self) -> Dict[str, Any]:
        """Cache istatistikleri"""
        return {
            "total_embeddings": len(self.cache),
            "cache_size_mb": (
                os.path.getsize(self.cache_file) / (1024 * 1024)
                if os.path.exists(self.cache_file)
                else 0
            ),
        }


class LocalEmbedder:
    """Local SentenceTransformer embedding sistemi"""

    # Desteklenen local modeller
    SUPPORTED_MODELS = {
        "paraphrase-multilingual-MiniLM-L12-v2": {"dimensions": 384, "max_tokens": 128},
        "paraphrase-multilingual-mpnet-base-v2": {"dimensions": 768, "max_tokens": 128},
        "all-MiniLM-L6-v2": {"dimensions": 384, "max_tokens": 256},
        "all-mpnet-base-v2": {"dimensions": 768, "max_tokens": 384},
    }

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "paraphrase-multilingual-MiniLM-L12-v2",
        enable_cache: bool = True,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        custom_dimensions: Optional[int] = None,
    ):
        self.model = model
        self.enable_cache = enable_cache
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.custom_dimensions = custom_dimensions
        
        # SentenceTransformer modelini yÃ¼kle
        print(f"ðŸ¤– SentenceTransformer modeli yÃ¼kleniyor: {self.model}")
        self.client = SentenceTransformer(self.model)
        
        # Cache'i baÅŸlat
        self.cache = EmbeddingCache() if enable_cache else None
        
        # Model bilgilerini al
        self.model_info = self.SUPPORTED_MODELS.get(model, {
            "dimensions": custom_dimensions or 384,
            "max_tokens": 128
        })
        
        logger.info(f"ðŸ¤– Local Embedder baÅŸlatÄ±ldÄ±")
        print(f"   Model: {self.model}")
        logger.info(f"   Dimensions: {self.model_info['dimensions']}")
        logger.info(f"   Max tokens: {self.model_info['max_tokens']}")
        logger.info(f"   Cache: {'Aktif' if self.cache else 'Devre dÄ±ÅŸÄ±'}")

    def _validate_text(self, text: str) -> str:
        """Metni valide et ve temizle"""
        if not text:
            return ""
        
        # String'e Ã§evir
        text = str(text).strip()
        
        # BoÅŸ string kontrolÃ¼
        if not text:
            return ""
        
        # Ã‡ok uzun metinleri kÄ±rp (model token limiti)
        if len(text) > 8000:  # Conservative limit
            logger.warning(f"Metin Ã§ok uzun, kÄ±rpÄ±lÄ±yor: {len(text)} -> 8000 karakter")
            text = text[:8000]
        
        return text

    def _call_embedding_api(self, texts: List[str], attempt: int = 1) -> List[np.ndarray]:
        """Local SentenceTransformer ile embedding Ã¼ret"""
        try:
            # Metinleri valide et
            validated_texts = []
            for text in texts:
                validated_text = self._validate_text(text)
                validated_texts.append(validated_text)
            
            # BoÅŸ metinleri filtrele
            non_empty_texts = [t for t in validated_texts if t]
            if not non_empty_texts:
                logger.warning("TÃ¼m metinler boÅŸ, sÄ±fÄ±r vektÃ¶rler dÃ¶ndÃ¼rÃ¼lÃ¼yor")
                dimension = self.model_info["dimensions"]
                return [np.zeros(dimension, dtype=np.float32) for _ in texts]
            
            # SentenceTransformer ile embedding Ã¼ret
            api_embeddings = self.client.encode(
                non_empty_texts,
                convert_to_numpy=True,
                show_progress_bar=False,
                normalize_embeddings=False
            )
            
            # SonuÃ§larÄ± liste olarak dÃ¶nÃ¼ÅŸtÃ¼r
            if len(api_embeddings.shape) == 1:
                api_embeddings = [api_embeddings]
            else:
                api_embeddings = [emb for emb in api_embeddings]
            
            # BoÅŸ metinler iÃ§in sÄ±fÄ±r vektÃ¶r ekle
            final_embeddings = []
            api_idx = 0
            
            for original_text in validated_texts:
                if original_text:  # BoÅŸ deÄŸilse model sonucunu kullan
                    if api_idx < len(api_embeddings):
                        final_embeddings.append(api_embeddings[api_idx].astype(np.float32))
                        api_idx += 1
                    else:
                        # Fallback
                        dimension = self.model_info["dimensions"]
                        final_embeddings.append(np.zeros(dimension, dtype=np.float32))
                else:  # BoÅŸ ise sÄ±fÄ±r vektÃ¶r
                    dimension = self.model_info["dimensions"]
                    final_embeddings.append(np.zeros(dimension, dtype=np.float32))
            
            return final_embeddings
            
        except Exception as e:
            logger.error(f"Embedding hatasÄ± (deneme {attempt}/{self.max_retries}): {e}")
            
            if attempt < self.max_retries:
                delay = self.retry_delay * (2 ** (attempt - 1))  # Exponential backoff
                logger.info(f"â³ {delay:.1f}s beklenip tekrar denenecek...")
                time.sleep(delay)
                return self._call_embedding_api(texts, attempt + 1)
            else:
                # Son deneme baÅŸarÄ±sÄ±z - sÄ±fÄ±r vektÃ¶r dÃ¶ndÃ¼r
                logger.error("Embedding Ã¼retimi baÅŸarÄ±sÄ±z, sÄ±fÄ±r vektÃ¶rler dÃ¶ndÃ¼rÃ¼lÃ¼yor")
                dimension = self.model_info["dimensions"]
                return [np.zeros(dimension, dtype=np.float32) for _ in texts]

    def embed_single(self, text: str, normalize: bool = False) -> np.ndarray:
        """Tek metin iÃ§in embedding"""
        validated_text = self._validate_text(text)
        
        if not validated_text:
            logger.warning("BoÅŸ metin iÃ§in sÄ±fÄ±r vektÃ¶r dÃ¶ndÃ¼rÃ¼lÃ¼yor")
            return np.zeros(self.model_info["dimensions"], dtype=np.float32)
        
        # Cache kontrolÃ¼
        if self.cache:
            cached_embedding = self.cache.get(validated_text, self.model)
            if cached_embedding is not None:
                logger.debug("Cache'den embedding alÄ±ndÄ±")
                return cached_embedding
        
        # API Ã§aÄŸrÄ±sÄ±
        embeddings = self._call_embedding_api([validated_text])
        embedding = embeddings[0]
        
        # Normalize (gerekirse)
        if normalize:
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
        
        # Cache'e kaydet
        if self.cache:
            self.cache.set(validated_text, self.model, embedding)
        
        return embedding

    def embed_batch(
        self,
        texts: List[str],
        batch_size: int = 50,  # Local iÃ§in batch size
        normalize: bool = False,
        show_progress: bool = True,
    ) -> List[np.ndarray]:
        """Batch embedding iÅŸlemi"""
        if not texts:
            logger.warning("BoÅŸ metin listesi")
            return []

        logger.info(f"ðŸ“Š {len(texts)} metin iÃ§in embedding hesaplanÄ±yor...")
        logger.info(f"Model: {self.model}, Batch size: {batch_size}")

        # Metinleri valide et
        validated_texts = []
        for i, text in enumerate(texts):
            validated_text = self._validate_text(text)
            validated_texts.append(validated_text)
            if not validated_text:
                logger.warning(f"BoÅŸ metin tespit edildi (index: {i})")

        # Cache kontrolÃ¼
        embeddings = []
        cached_results = {}
        texts_to_process = []
        text_indices = []

        if self.cache:
            cache_hits = 0
            for i, text in enumerate(validated_texts):
                if not text:
                    cached_results[i] = np.zeros(self.model_info["dimensions"], dtype=np.float32)
                else:
                    cached = self.cache.get(text, self.model)
                    if cached is not None:
                        cached_results[i] = cached
                        cache_hits += 1
                    else:
                        texts_to_process.append(text)
                        text_indices.append(i)
            
            logger.info(f"ðŸ’¾ Cache hits: {cache_hits}/{len(validated_texts)}")
        else:
            texts_to_process = []
            text_indices = []
            for i, text in enumerate(validated_texts):
                if not text:
                    cached_results[i] = np.zeros(self.model_info["dimensions"], dtype=np.float32)
                else:
                    texts_to_process.append(text)
                    text_indices.append(i)

        # API Ã§aÄŸrÄ±larÄ±
        if texts_to_process:
            logger.info(f"ðŸ”„ API ile iÅŸlenecek metin sayÄ±sÄ±: {len(texts_to_process)}")
            progress_bar = tqdm(total=len(texts_to_process), desc="Embedding", disable=not show_progress)
            
            for i in range(0, len(texts_to_process), batch_size):
                batch = texts_to_process[i:i + batch_size]
                logger.debug(f"Batch iÅŸleniyor: {i//batch_size + 1}/{(len(texts_to_process)-1)//batch_size + 1}")
                
                batch_embeddings = self._call_embedding_api(batch)
                
                # SonuÃ§larÄ± kaydet
                for j, embedding in enumerate(batch_embeddings):
                    if i + j < len(text_indices):
                        text_idx = text_indices[i + j]
                        text = texts_to_process[i + j]
                        
                        # Embedding kontrolÃ¼
                        if embedding is None or np.isnan(embedding).any():
                            logger.warning(f"GeÃ§ersiz embedding tespit edildi (index: {text_idx})")
                            embedding = np.zeros(self.model_info["dimensions"], dtype=np.float32)
                        
                        # Normalize (gerekirse)
                        if normalize:
                            norm = np.linalg.norm(embedding)
                            if norm > 0:
                                embedding = embedding / norm
                        
                        # Cache'e kaydet
                        if self.cache and text:
                            self.cache.set(text, self.model, embedding)
                        
                        cached_results[text_idx] = embedding
                
                progress_bar.update(len(batch))
                
                # Rate limiting (Local processing)
                time.sleep(0.1)
            
            progress_bar.close()

        # SonuÃ§larÄ± sÄ±rala
        final_embeddings = []
        for i in range(len(texts)):
            if i in cached_results:
                final_embeddings.append(cached_results[i])
            else:
                logger.warning(f"Embedding eksik (index: {i}), sÄ±fÄ±r vektÃ¶r ekleniyor")
                final_embeddings.append(np.zeros(self.model_info["dimensions"], dtype=np.float32))

        logger.info(f"âœ… {len(final_embeddings)} embedding hazÄ±rlandÄ±")
        return final_embeddings

    def embed_with_ensemble(
        self,
        texts: List[str],
        models: Optional[List[str]] = None,
        weights: Optional[List[float]] = None,
    ) -> List[np.ndarray]:
        """Ensemble embedding (Ã§oklu model birleÅŸtirme)"""
        models = models or ["text-embedding-3-small", "text-embedding-ada-002"]
        weights = weights or [1.0] * len(models)

        if len(models) != len(weights):
            raise ValueError("Model sayÄ±sÄ± ve aÄŸÄ±rlÄ±k sayÄ±sÄ± eÅŸit olmalÄ±")

        logger.info(f"ðŸ”— Ensemble embedding: {len(models)} model")

        all_embeddings = []
        original_model = self.model

        # Her model iÃ§in embedding hesapla
        for model_name, weight in zip(models, weights):
            logger.info(f"ðŸ“Š Model iÅŸleniyor: {model_name} (aÄŸÄ±rlÄ±k: {weight})")
            
            # GeÃ§ici olarak modeli deÄŸiÅŸtir
            self.model = model_name
            self.model_info = self.SUPPORTED_MODELS.get(model_name, {
                "dimensions": 1536,
                "max_tokens": 8192
            })
            
            model_embeddings = self.embed_batch(texts, show_progress=False)
            all_embeddings.append((model_embeddings, weight))

        # Orijinal modeli geri yÃ¼kle
        self.model = original_model
        self.model_info = self.SUPPORTED_MODELS.get(original_model, {
            "dimensions": 1536,
            "max_tokens": 8192
        })

        # AÄŸÄ±rlÄ±klÄ± ortalama
        logger.info("ðŸ”„ Ensemble birleÅŸtirme yapÄ±lÄ±yor...")
        ensemble_embeddings = []

        for i in range(len(texts)):
            combined_embedding = None
            total_weight = 0

            for model_embeddings, weight in all_embeddings:
                if i < len(model_embeddings) and model_embeddings[i] is not None:
                    if combined_embedding is None:
                        combined_embedding = model_embeddings[i] * weight
                    else:
                        # Boyut uyumsuzluÄŸu kontrolÃ¼
                        if len(combined_embedding) != len(model_embeddings[i]):
                            logger.warning(f"Boyut uyumsuzluÄŸu: {len(combined_embedding)} vs {len(model_embeddings[i])}")
                            continue
                        combined_embedding += model_embeddings[i] * weight
                    total_weight += weight

            if combined_embedding is not None and total_weight > 0:
                combined_embedding /= total_weight
                # Normalize
                norm = np.linalg.norm(combined_embedding)
                if norm > 0:
                    combined_embedding /= norm
                ensemble_embeddings.append(combined_embedding)
            else:
                # Fallback: sÄ±fÄ±r vektÃ¶r
                ensemble_embeddings.append(np.zeros(self.model_info["dimensions"], dtype=np.float32))

        logger.info("âœ… Ensemble embedding tamamlandÄ±")
        return ensemble_embeddings

    def benchmark_models(
        self, test_texts: Optional[List[str]] = None
    ) -> Dict[str, Dict[str, float]]:
        """Model performance benchmark"""
        test_texts = test_texts or [
            "Bu bir test cÃ¼mlesidir.",
            "Merhaba dÃ¼nya! NasÄ±lsÄ±n?",
            "Eduroam aÄŸÄ±na nasÄ±l baÄŸlanabilirim?",
        ]

        logger.info("ðŸƒ Model benchmark baÅŸlÄ±yor...")
        
        results = {}
        original_model = self.model

        for model_name in self.SUPPORTED_MODELS.keys():
            logger.info(f"â±ï¸ Test ediliyor: {model_name}")

            try:
                # GeÃ§ici olarak modeli deÄŸiÅŸtir
                self.model = model_name
                self.model_info = self.SUPPORTED_MODELS[model_name]
                
                start_time = time.time()

                # Test embedding
                embeddings = self.embed_batch(test_texts, show_progress=False)

                end_time = time.time()
                duration = end_time - start_time

                # Embedding kalitesi (basit metrik)
                valid_embeddings = [emb for emb in embeddings if emb is not None]
                if valid_embeddings:
                    avg_norm = np.mean([np.linalg.norm(emb) for emb in valid_embeddings])
                else:
                    avg_norm = 0.0

                results[model_name] = {
                    "duration_seconds": duration,
                    "texts_per_second": len(test_texts) / duration if duration > 0 else 0,
                    "avg_embedding_norm": float(avg_norm),
                    "embedding_dimension": self.model_info["dimensions"],
                    "max_tokens": self.model_info["max_tokens"],
                    "valid_embeddings": len(valid_embeddings),
                }

                logger.info(f"   âš¡ {len(test_texts)/duration:.1f} text/sec, dim: {self.model_info['dimensions']}")

            except Exception as e:
                logger.error(f"   âŒ Benchmark hatasÄ± {model_name}: {e}")
                results[model_name] = {"error": str(e)}

        # Orijinal modeli geri yÃ¼kle
        self.model = original_model
        self.model_info = self.SUPPORTED_MODELS.get(original_model, {
            "dimensions": 1536,
            "max_tokens": 8192
        })

        return results

    def get_model_info(self) -> Dict[str, Any]:
        """Model bilgileri"""
        info = {
            "model": self.model,
            "dimensions": self.model_info["dimensions"],
            "max_tokens": self.model_info["max_tokens"],
            "custom_dimensions": self.custom_dimensions,
            "cache_enabled": self.cache is not None,
            "supported_models": list(self.SUPPORTED_MODELS.keys()),
        }

        if self.cache:
            info["cache_stats"] = self.cache.get_stats()

        return info

    def cleanup(self):
        """Cleanup iÅŸlemleri"""
        if self.cache:
            self.cache._save_cache()
        logger.info("ðŸ§¹ Cleanup tamamlandÄ±")


def validate_input_data(data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Input verisini valide et"""
    stats = {
        "total_documents": len(data),
        "documents_with_chunks": 0,
        "total_chunks": 0,
        "empty_chunks": 0,
        "valid_chunks": 0,
        "issues": []
    }
    
    for doc_idx, item in enumerate(data):
        if not isinstance(item, dict):
            stats["issues"].append(f"DokÃ¼man {doc_idx}: Dict deÄŸil")
            continue
            
        chunks = item.get("chunks", [])
        if not chunks:
            stats["issues"].append(f"DokÃ¼man {doc_idx}: Chunk'lar boÅŸ")
            continue
            
        if not isinstance(chunks, list):
            stats["issues"].append(f"DokÃ¼man {doc_idx}: Chunks liste deÄŸil")
            continue
            
        stats["documents_with_chunks"] += 1
        stats["total_chunks"] += len(chunks)
        
        # Chunk'larÄ± kontrol et
        for chunk_idx, chunk in enumerate(chunks):
            if not chunk or not str(chunk).strip():
                stats["empty_chunks"] += 1
                stats["issues"].append(f"DokÃ¼man {doc_idx}, Chunk {chunk_idx}: BoÅŸ")
            else:
                stats["valid_chunks"] += 1
    
    return stats


def process_documents_with_embeddings(
    input_file: str, output_file: str, model_config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """DokÃ¼manlarÄ± embedding'lerle iÅŸle"""
    print("ðŸš€ Embedding iÅŸlemi baÅŸlÄ±yor...")
    # Default config
    default_config = {
        "model": "paraphrase-multilingual-MiniLM-L12-v2",
        "custom_dimensions": None,
        "use_ensemble": False,
        "ensemble_models": None,
        "batch_size": 50,
        "use_cache": True,
        "max_retries": 3,
        "retry_delay": 1.0,
        "validate_input": True,
    }

    if model_config:
        default_config.update(model_config)

    print("ðŸš€ Embedding iÅŸlemi baÅŸlÄ±yor...")
    print(f"ðŸ“„ Girdi: {input_file}")
    print(f"ðŸ’¾ Ã‡Ä±ktÄ±: {output_file}")

    # Input dosyasÄ±nÄ± yÃ¼kle
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ Input dosyasÄ± okunamadÄ±: {e}")
        raise

    if not data:
        logger.error("âŒ Input dosyasÄ± boÅŸ")
        return {"error": "Empty input file"}

    # Input verisini valide et
    if default_config["validate_input"]:
        logger.info("ðŸ” Input verisi valide ediliyor...")
        validation_stats = validate_input_data(data)
        
        logger.info("ðŸ“Š VERÄ° VALÄ°DASYON SONUÃ‡LARI:")
        logger.info(f"   Toplam dokÃ¼man: {validation_stats['total_documents']}")
        logger.info(f"   Chunk'lÄ± dokÃ¼man: {validation_stats['documents_with_chunks']}")
        logger.info(f"   Toplam chunk: {validation_stats['total_chunks']}")
        logger.info(f"   GeÃ§erli chunk: {validation_stats['valid_chunks']}")
        logger.info(f"   BoÅŸ chunk: {validation_stats['empty_chunks']}")
        
        if validation_stats["issues"]:
            logger.warning(f"âš ï¸ {len(validation_stats['issues'])} sorun tespit edildi")
            for issue in validation_stats["issues"][:10]:  # Ä°lk 10 sorunu gÃ¶ster
                logger.warning(f"   - {issue}")
            if len(validation_stats["issues"]) > 10:
                logger.warning(f"   ... ve {len(validation_stats['issues']) - 10} sorun daha")

    # Embedder oluÅŸtur
    embedder = LocalEmbedder(
        model=default_config["model"],
        enable_cache=default_config["use_cache"],
        max_retries=default_config["max_retries"],
        retry_delay=default_config["retry_delay"],
        custom_dimensions=default_config["custom_dimensions"],
    )

    # Ä°statistikler
    total_documents = len(data)
    total_chunks = sum(len(item.get("chunks", [])) for item in data)
    processed_chunks = 0
    successful_documents = 0

    print(f"ðŸ“Š {total_documents} dokÃ¼man, {total_chunks} chunk iÅŸlenecek")

    try:
        for doc_idx, item in enumerate(data, 1):
            chunks = item.get("chunks", [])
            if not chunks:
                logger.warning(f"âš ï¸ DokÃ¼man {doc_idx} chunk'larÄ± boÅŸ, atlanÄ±yor")
                item["embeddings"] = []
                continue

            filename = item.get("filename", f"Document_{doc_idx}")
            logger.info(f"[{doc_idx}/{total_documents}] Ä°ÅŸleniyor: {filename}")

            try:
                # Embedding hesapla
                if default_config["use_ensemble"] and default_config["ensemble_models"]:
                    embeddings = embedder.embed_with_ensemble(
                        chunks, models=default_config["ensemble_models"]
                    )
                else:
                    embeddings = embedder.embed_batch(
                        chunks, 
                        batch_size=default_config["batch_size"], 
                        show_progress=False
                    )

                # Numpy array'leri liste'ye Ã§evir
                embeddings_list = []
                for emb in embeddings:
                    if emb is not None:
                        embeddings_list.append(emb.tolist())
                    else:
                        # Fallback: sÄ±fÄ±r vektÃ¶r
                        embeddings_list.append([0.0] * embedder.model_info["dimensions"])

                item["embeddings"] = embeddings_list
                processed_chunks += len(chunks)
                successful_documents += 1

                print(f"   âœ… {len(chunks)} chunk embedding tamamlandÄ±")

            except Exception as e:
                logger.error(f"   âŒ DokÃ¼man {doc_idx} embedding hatasÄ±: {e}")
                # Fallback: boÅŸ embedding listesi
                item["embeddings"] = [[0.0] * embedder.model_info["dimensions"]] * len(chunks)

        # SonuÃ§larÄ± kaydet
        logger.info("ðŸ’¾ SonuÃ§lar kaydediliyor...")

        # Backup eski dosya
        if os.path.exists(output_file):
            backup_file = f"{output_file}.backup_{int(time.time())}"
            os.rename(output_file, backup_file)
            logger.info(f"ðŸ“ Backup oluÅŸturuldu: {backup_file}")

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        # Dosya boyutu kontrolÃ¼
        file_size = os.path.getsize(output_file) / (1024 * 1024)
        print(f"âœ… Embedding dosyasÄ± kaydedildi: {output_file} ({file_size:.2f} MB)")

        # Final istatistikler
        stats = {
            "total_documents": total_documents,
            "successful_documents": successful_documents,
            "total_chunks": total_chunks,
            "processed_chunks": processed_chunks,
            "output_file_size_mb": file_size,
            "model_info": embedder.get_model_info(),
            "success_rate": (successful_documents / total_documents) * 100 if total_documents > 0 else 0,
        }

        print("ðŸ“Š EMBEDDING Ä°STATÄ°STÄ°KLERÄ°:")
        print(f"   Ä°ÅŸlenen dokÃ¼man: {successful_documents}/{total_documents}")
        print(f"   Ä°ÅŸlenen chunk: {processed_chunks}")
        print(f"   Dosya boyutu: {file_size:.2f} MB")
        print(f"   Model: {default_config['model']}")
        print(f"   BaÅŸarÄ± oranÄ±: {stats['success_rate']:.1f}%")

        # Cleanup
        embedder.cleanup()

        return stats

    except Exception as e:
        logger.error(f"âŒ Embedding iÅŸlemi hatasÄ±: {e}")
        embedder.cleanup()
        raise


def main():
    """uploads_base.json'daki verileri embed ederek uploads_with_embed.json'a kaydeder"""
    input_file = "uploads_base.json"
    output_file = "uploads_with_embed.json"

    embedding_config = {
        "model": "text-embedding-3-small",  # Veya "text-embedding-3-large"
        "custom_dimensions": None,  # VarsayÄ±lan boyut kullan
        "use_ensemble": False,
        "ensemble_models": ["paraphrase-multilingual-MiniLM-L12-v2"],
        "batch_size": 50,
        "use_cache": True,
        "max_retries": 3,
        "retry_delay": 1.0,
        "validate_input": True,
    }

    logging.basicConfig(
        level=logging.INFO, 
        format="%(asctime)s - %(levelname)s - %(message)s"
    )

    try:
        logger.info("ðŸš€ uploads_base.json embedding iÅŸlemi baÅŸlÄ±yor...")
        stats = process_documents_with_embeddings(
            input_file=input_file,
            output_file=output_file,
            model_config=embedding_config,
        )
        logger.info(f"âœ… uploads_with_embed.json kaydedildi. Ä°statistikler: {stats}")
    except Exception as e:
        logger.error(f"âŒ Ana iÅŸlem hatasÄ±: {e}")
        raise


# Ek yardÄ±mcÄ± fonksiyonlar
def debug_input_file(input_file: str) -> Dict[str, Any]:
    """Input dosyasÄ±nÄ± detaylÄ± ÅŸekilde debug et"""
    logger.info(f"ðŸ” Debug ediliyor: {input_file}")
    
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        logger.error(f"âŒ Dosya okunamadÄ±: {e}")
        return {"error": str(e)}
    
    debug_info = {
        "file_exists": os.path.exists(input_file),
        "file_size_mb": os.path.getsize(input_file) / (1024 * 1024),
        "data_type": type(data).__name__,
        "data_length": len(data) if hasattr(data, '__len__') else "N/A",
    }
    
    if isinstance(data, list):
        debug_info["sample_documents"] = []
        for i, item in enumerate(data[:3]):  # Ä°lk 3 dokÃ¼manÄ± incele
            doc_info = {
                "index": i,
                "type": type(item).__name__,
                "keys": list(item.keys()) if isinstance(item, dict) else "N/A",
            }
            
            if isinstance(item, dict):
                chunks = item.get("chunks", [])
                doc_info["chunks_count"] = len(chunks)
                doc_info["chunks_type"] = type(chunks).__name__
                
                if chunks:
                    doc_info["sample_chunks"] = []
                    for j, chunk in enumerate(chunks[:2]):  # Ä°lk 2 chunk'Ä± incele
                        chunk_info = {
                            "index": j,
                            "type": type(chunk).__name__,
                            "length": len(str(chunk)) if chunk else 0,
                            "preview": str(chunk)[:100] if chunk else "EMPTY",
                        }
                        doc_info["sample_chunks"].append(chunk_info)
            
            debug_info["sample_documents"].append(doc_info)
    
    logger.info("ðŸ“Š DEBUG SONUÃ‡LARI:")
    logger.info(f"   Dosya boyutu: {debug_info['file_size_mb']:.2f} MB")
    logger.info(f"   Veri tipi: {debug_info['data_type']}")
    logger.info(f"   Veri uzunluÄŸu: {debug_info['data_length']}")
    
    return debug_info


def repair_input_file(input_file: str, output_file: str) -> Dict[str, Any]:
    """Bozuk input dosyasÄ±nÄ± onar"""
    logger.info(f"ðŸ”§ OnarÄ±lÄ±yor: {input_file} -> {output_file}")
    
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        logger.error(f"âŒ Dosya okunamadÄ±: {e}")
        return {"error": str(e)}
    
    if not isinstance(data, list):
        logger.error("âŒ Veri liste formatÄ±nda deÄŸil")
        return {"error": "Data is not a list"}
    
    repaired_data = []
    repair_stats = {
        "original_documents": len(data),
        "repaired_documents": 0,
        "removed_documents": 0,
        "fixed_chunks": 0,
        "removed_chunks": 0,
    }
    
    for doc_idx, item in enumerate(data):
        if not isinstance(item, dict):
            logger.warning(f"âš ï¸ DokÃ¼man {doc_idx}: Dict deÄŸil, atlanÄ±yor")
            repair_stats["removed_documents"] += 1
            continue
        
        # Chunks kontrolÃ¼
        chunks = item.get("chunks", [])
        if not isinstance(chunks, list):
            logger.warning(f"âš ï¸ DokÃ¼man {doc_idx}: Chunks liste deÄŸil, dÃ¼zeltiliyor")
            chunks = []
        
        # Chunk'larÄ± temizle
        cleaned_chunks = []
        for chunk_idx, chunk in enumerate(chunks):
            if chunk and str(chunk).strip():
                cleaned_chunks.append(str(chunk).strip())
                repair_stats["fixed_chunks"] += 1
            else:
                repair_stats["removed_chunks"] += 1
        
        # TemizlenmiÅŸ chunks'Ä± kaydet
        item["chunks"] = cleaned_chunks
        
        # DokÃ¼manÄ± kaydet (boÅŸ chunks olsa bile)
        repaired_data.append(item)
        repair_stats["repaired_documents"] += 1
    
    # OnarÄ±lmÄ±ÅŸ dosyayÄ± kaydet
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(repaired_data, f, ensure_ascii=False, indent=2)
    
    logger.info("ðŸ”§ ONARIM SONUÃ‡LARI:")
    logger.info(f"   Orijinal dokÃ¼man: {repair_stats['original_documents']}")
    logger.info(f"   OnarÄ±lan dokÃ¼man: {repair_stats['repaired_documents']}")
    logger.info(f"   KaldÄ±rÄ±lan dokÃ¼man: {repair_stats['removed_documents']}")
    logger.info(f"   DÃ¼zeltilen chunk: {repair_stats['fixed_chunks']}")
    logger.info(f"   KaldÄ±rÄ±lan chunk: {repair_stats['removed_chunks']}")
    
    return repair_stats


def test_embedding_system():
    """Embedding sistemini test et"""
    logger.info("ðŸ§ª Embedding sistemi test ediliyor...")
    
    # Test verileri
    test_data = [
        {
            "filename": "test_doc_1.txt",
            "chunks": [
                "Bu bir test cÃ¼mlesidir.",
                "Ä°kinci test cÃ¼mlesi.",
                "",  # BoÅŸ chunk
                "ÃœÃ§Ã¼ncÃ¼ test cÃ¼mlesi.",
            ]
        },
        {
            "filename": "test_doc_2.txt",
            "chunks": []  # BoÅŸ chunks
        },
        {
            "filename": "test_doc_3.txt",
            "chunks": [
                "DÃ¶rdÃ¼ncÃ¼ test cÃ¼mlesi.",
                "BeÅŸinci test cÃ¼mlesi.",
            ]
        }
    ]
    
    # Test dosyalarÄ±nÄ± oluÅŸtur
    test_input = "test_input.json"
    test_output = "test_output.json"
    
    with open(test_input, "w", encoding="utf-8") as f:
        json.dump(test_data, f, ensure_ascii=False, indent=2)
    
    # Test config
    test_config = {
        "model": "text-embedding-3-small",
        "batch_size": 10,
        "use_cache": True,
        "max_retries": 2,
        "validate_input": True,
    }
    
    try:
        # Test Ã§alÄ±ÅŸtÄ±r
        stats = process_documents_with_embeddings(
            input_file=test_input,
            output_file=test_output,
            model_config=test_config,
        )
        
        logger.info("âœ… Test baÅŸarÄ±lÄ±!")
        logger.info(f"Test sonuÃ§larÄ±: {stats}")
        
        # Temizlik
        if os.path.exists(test_input):
            os.remove(test_input)
        if os.path.exists(test_output):
            os.remove(test_output)
            
        return stats
        
    except Exception as e:
        logger.error(f"âŒ Test hatasÄ±: {e}")
        # Temizlik
        if os.path.exists(test_input):
            os.remove(test_input)
        if os.path.exists(test_output):
            os.remove(test_output)
        raise


if __name__ == "__main__":
    # Ana fonksiyon veya debug/test fonksiyonlarÄ± Ã§alÄ±ÅŸtÄ±r
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "debug":
            input_file = sys.argv[2] if len(sys.argv) > 2 else "uploads_base.json"
            debug_input_file(input_file)
            
        elif command == "repair":
            input_file = sys.argv[2] if len(sys.argv) > 2 else "uploads_base.json"
            output_file = sys.argv[3] if len(sys.argv) > 3 else "uploads_base_repaired.json"
            repair_input_file(input_file, output_file)
            
        elif command == "test":
            test_embedding_system()
            
        else:
            logger.error(f"Bilinmeyen komut: {command}")
            logger.info("KullanÄ±m: python script.py [debug|repair|test] [input_file] [output_file]")
    else:
        # Normal Ã§alÄ±ÅŸtÄ±rma
        main()