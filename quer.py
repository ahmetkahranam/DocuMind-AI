import requests
import re
import json
from typing import Dict, Any, Optional, List
from config import config
import logging
import ollama

logger = logging.getLogger(__name__)


def temizle_yanit(yazi: str) -> str:
    """YanÄ±tÄ± temizle ve dÃ¼zenle"""
    if not yazi:
        return ""

    # <think> etiketlerini kaldÄ±r
    yazi = re.sub(r"<think>.*?</think>", "", yazi, flags=re.DOTALL | re.IGNORECASE)

    # HTML etiketlerini kaldÄ±r
    yazi = re.sub(r"<.*?>", "", yazi, flags=re.DOTALL)

    # Markdown iÅŸaretlerini temizle
    yazi = re.sub(r"\*\*|__|~~|`", "", yazi)

    # Eksik parantezleri temizle - satÄ±r sonunda tek parantez
    yazi = re.sub(r'\s*\($\s*', '', yazi, flags=re.MULTILINE)  # SatÄ±r sonunda tek aÃ§Ä±k parantez
    yazi = re.sub(r'\s*\)$\s*', '', yazi, flags=re.MULTILINE)  # SatÄ±r sonunda tek kapalÄ± parantez
    yazi = re.sub(r'\s*\[\s*$', '', yazi, flags=re.MULTILINE)  # SatÄ±r sonunda tek aÃ§Ä±k kÃ¶ÅŸeli parantez
    yazi = re.sub(r'\s*\]\s*$', '', yazi, flags=re.MULTILINE)  # SatÄ±r sonunda tek kapalÄ± kÃ¶ÅŸeli parantez
    
    # Eksik noktalama iÅŸaretlerini temizle
    yazi = re.sub(r'\s*[,;:]\s*$', '', yazi, flags=re.MULTILINE)  # SatÄ±r sonunda virgÃ¼l, noktalÄ± virgÃ¼l, iki nokta

    # Ã‡oklu boÅŸluklarÄ± temizle
    yazi = re.sub(r"\s+", " ", yazi).strip()

    # Ã‡ok uzun yanÄ±tlarÄ± kÄ±salt
    if len(yazi) > config.MAX_ANSWER_LENGTH:
        words = yazi.split()
        yazi = " ".join(words[: config.MAX_ANSWER_LENGTH // 5]) + "..."

    return yazi


def enhanced_prompt_engineering(prompt: str, query_category: str = "general") -> str:
    """GeliÅŸmiÅŸ prompt mÃ¼hendisliÄŸi"""

    # Kategori bazlÄ± ek talimatlar
    category_instructions = {
        "procedure": "\nAdÄ±m adÄ±m aÃ§Ä±klama yapÄ±n. SÄ±ralÄ± liste halinde sunun.",
        "temporal": "\nTarih ve zaman bilgilerini kesin olarak belirtin.",
        "quantitative": "\nSayÄ±sal bilgileri tam ve doÄŸru verin.",
        "definition": "\nTanÄ±mlarÄ± aÃ§Ä±k ve anlaÅŸÄ±lÄ±r yapÄ±n.",
        "explanation": "\nSebep-sonuÃ§ iliÅŸkilerini aÃ§Ä±klayÄ±n.",
        "location": "\nYer bilgilerini spesifik belirtin.",
        "general": "\nKapsamlÄ± ve dÃ¼zenli aÃ§Ä±klama yapÄ±n.",
    }

    enhanced_prompt = prompt + category_instructions.get(query_category, "")

    # Maksimum gÃ¼Ã§lÃ¼ prompt talimatlarÄ±
    quality_instructions = """

Ã‡Ã–ZÃœLMEZ PROMPT TALÄ°MATLARI:
- Verilen belgelerdeki HER METÄ°N PARÃ‡ASÄ±nÄ± tamamen tara ve oku
- SayÄ±lÄ± madde ve kurallarÄ± mutlaka bul ve belirt
- Anahtar kelimeleri dikkatli ara
- Madde numaralarÄ±nÄ± (Ã¶rn: 14. madde) MUTLAKA belirt  
- Kesin ve doÄŸrudan yanÄ±t ver - "galiba, sanÄ±rÄ±m" YASAK
- SADECE gerÃ§ekten hiÃ§ bilgi yoksa "belirtilmemiÅŸ" de
- Belgede bilgi varsa "belirtilmemiÅŸ" deme - bu YANLIÅ
- Her belge parÃ§asÄ±nda detaylÄ± arama yapman GEREKÄ°YOR"""

    return enhanced_prompt + quality_instructions


def ask_local_llm(
    prompt: str,
    model: Optional[str] = None,
    query_category: str = "general",
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> str:
    """Ollama yerel LLM kullanarak Ã§aÄŸrÄ±"""

    if model is None:
        model = config.LLM_MODEL
    if temperature is None:
        temperature = config.LLM_TEMPERATURE
    if max_tokens is None:
        max_tokens = config.LLM_MAX_TOKENS

    # Prompt'u geliÅŸtir
    enhanced_prompt = enhanced_prompt_engineering(prompt, query_category)

    try:
        logger.info(f"ğŸ”„ Ollama LLM Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor - Model: {model}")

        # Ollama API Ã§aÄŸrÄ±sÄ±
        response = ollama.chat(
            model=model,
            messages=[{"role": "user", "content": enhanced_prompt}],
            options={
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        )

        raw_yanit = response['message']['content']

        if not raw_yanit:
            logger.warning("âš ï¸ Ollama'dan boÅŸ yanÄ±t alÄ±ndÄ±")
            return "âš ï¸ Modelden net bir yanÄ±t alÄ±namadÄ±."

        # YanÄ±tÄ± temizle
        temiz_yanit = temizle_yanit(raw_yanit)

        # Minimum uzunluk kontrolÃ¼
        if len(temiz_yanit) < config.MIN_ANSWER_LENGTH:
            logger.warning(f"âš ï¸ Ã‡ok kÄ±sa yanÄ±t: {len(temiz_yanit)} karakter")
            return (
                "âš ï¸ Yeterince detaylÄ± yanÄ±t alÄ±namadÄ±. LÃ¼tfen daha spesifik soru sorun."
            )

        logger.info(f"âœ… Ollama yanÄ±tÄ± alÄ±ndÄ± - {len(temiz_yanit)} karakter")
        return temiz_yanit

    except Exception as e:
        logger.error(f"âš ï¸ Ollama LLM hatasÄ±: {e}")
        if "not found" in str(e).lower() or "model" in str(e).lower():
            return f"âš ï¸ Model '{model}' bulunamadÄ±. LÃ¼tfen 'ollama pull {model}' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n."
        elif "connection" in str(e).lower():
            return "âš ï¸ Ollama servisine baÄŸlanÄ±lamadÄ±. LÃ¼tfen 'ollama serve' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n."
        else:
            return f"âš ï¸ Yerel LLM hatasÄ±: {str(e)[:100]}"


def batch_llm_requests(
    prompts_list: List[Any], model: Optional[str] = None
) -> List[str]:
    """Ã‡oklu LLM istekleri iÃ§in batch iÅŸleme"""
    results = []

    for i, prompt_data in enumerate(prompts_list):
        if isinstance(prompt_data, dict):
            prompt = prompt_data.get("prompt", "")
            category = prompt_data.get("category", "general")
        else:
            prompt = str(prompt_data)
            category = "general"

        logger.info(f"ğŸ”„ Batch iÅŸlem {i+1}/{len(prompts_list)}")

        result = ask_local_llm(prompt, model=model, query_category=category)
        results.append(result)

    return results


def validate_llm_connection() -> Dict[str, Any]:
    """Ollama baÄŸlantÄ±sÄ±nÄ± doÄŸrula"""
    try:
        logger.info("ğŸ”„ Ollama baÄŸlantÄ±sÄ± test ediliyor...")

        # Mevcut modelleri al
        models_response = ollama.list()
        available_models = [model['name'] for model in models_response.get('models', [])]

        # Test Ã§aÄŸrÄ±sÄ±
        response = ollama.chat(
            model=config.LLM_MODEL,
            messages=[{"role": "user", "content": "Test"}],
            options={"num_predict": 5}
        )

        target_model = config.LLM_MODEL
        model_available = any(target_model in model for model in available_models)

        return {
            "connected": True,
            "service": "Ollama Local LLM",
            "available_models": available_models,
            "target_model": target_model,
            "target_model_available": model_available,
            "total_models": len(available_models),
        }

    except Exception as e:
        error_msg = str(e).lower()
        if "connection" in error_msg or "refused" in error_msg:
            return {
                "connected": False,
                "error": "Ollama servisine baÄŸlanÄ±lamadÄ±",
                "suggestion": "'ollama serve' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n",
            }
        elif "not found" in error_msg or "model" in error_msg:
            return {
                "connected": False,
                "error": f"Model '{config.LLM_MODEL}' bulunamadÄ±",
                "suggestion": f"'ollama pull {config.LLM_MODEL}' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n",
            }
        else:
            return {
                "connected": False,
                "error": f"Ollama hatasÄ±: {str(e)[:100]}",
                "suggestion": "Ollama kurulumunu kontrol edin",
            }


def test_llm_quality() -> Dict[str, Any]:
    """LLM yanÄ±t kalitesini test et"""
    test_prompts = [
        {
            "prompt": "Sistem hakkÄ±nda kÄ±sa bilgi ver.",
            "category": "general",
            "expected_keywords": ["bilgi", "sistem", "dokÃ¼man"],
        },
        {
            "prompt": "Ä°ÅŸlem adÄ±mlarÄ± nelerdir?",
            "category": "procedure",
            "expected_keywords": ["adÄ±m", "iÅŸlem", "prosedÃ¼r"],
        },
    ]

    results = []

    for test in test_prompts:
        response = ask_local_llm(test["prompt"], query_category=test["category"])

        # Keyword kontrolÃ¼
        keyword_found = any(
            keyword in response.lower() for keyword in test["expected_keywords"]
        )

        results.append(
            {
                "prompt": test["prompt"],
                "response": response[:100] + "...",  # Ä°lk 100 karakter
                "length": len(response),
                "keywords_found": keyword_found,
                "quality": "Good" if keyword_found and len(response) > 50 else "Poor",
            }
        )

    return {
        "tests": results,
        "passed": sum(1 for r in results if r["quality"] == "Good"),
        "total": len(results),
    }


if __name__ == "__main__":
    # Test iÅŸlemleri
    print("ğŸ§ª Ollama BaÄŸlantÄ± Testi...")
    connection_status = validate_llm_connection()
    print(f"BaÄŸlantÄ± durumu: {connection_status}")

    if connection_status.get("connected"):
        print("\nğŸ§ª LLM Kalite Testi...")
        quality_results = test_llm_quality()
        print(f"GeÃ§en testler: {quality_results['passed']}/{quality_results['total']}")

        for test in quality_results["tests"]:
            print(f"âœ… {test['prompt'][:30]}... - Kalite: {test['quality']}")
    else:
        print("âŒ Ollama baÄŸlantÄ±sÄ± kurulamadÄ±, kalite testi yapÄ±lamÄ±yor.")
        print(f"Hata: {connection_status.get('error', 'Bilinmeyen hata')}")
        print(
            f"Ã–neri: {connection_status.get('suggestion', 'KonfigÃ¼rasyonu kontrol edin')}"
        )