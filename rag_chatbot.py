import chromadb
from quer import ask_local_llm, temizle_yanit
from config import config
from query_processor import QueryProcessor
from hybrid_retriever import HybridRetriever
from evaluator import ResponseEvaluator
import logging
import re
from typing import Dict, List, Any, Optional

# Logging setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AdvancedRAGChatbot:
    """GeliÅŸmiÅŸ RAG Chatbot sistemi"""

    def __init__(self, chroma_path: str = "./chroma"):
        self.retriever = HybridRetriever(chroma_path)
        self.query_processor = QueryProcessor()
        self.evaluator = ResponseEvaluator()
        
        # Conversation history iÃ§in
        self.conversation_history = []  # [(question, answer, timestamp), ...]
        self.max_history_length = 5  # Son 5 soru-cevap Ã§iftini hatÄ±rla

        logger.info("ğŸ¤– GeliÅŸmiÅŸ RAG Chatbot baÅŸlatÄ±ldÄ±!")

    def process_query(self, user_query: str) -> Dict[str, Any]:
        """KullanÄ±cÄ± sorgusunu kapsamlÄ± ÅŸekilde iÅŸle"""
        try:
            # 1. Context-aware query expansion
            expanded_query = self._expand_query_with_context(user_query)
            
            # 2. Query preprocessing
            processed_query = self.query_processor.process_query(expanded_query)
            logger.info(f"ğŸ“ Ä°ÅŸlenmiÅŸ sorgu kategorisi: {processed_query['category']}")

            # 3. Advanced retrieval
            retrieval_result = self.retriever.advanced_retrieve(
                expanded_query, n_results=config.DEFAULT_N_RESULTS
            )

            if not retrieval_result["results"]:
                return self._handle_no_results(user_query)

            # 3.5. Context-aware boosting - conversation history'deki belgeleri Ã¶ne Ã§Ä±kar
            boosted_results = self._apply_conversation_context_boost(retrieval_result["results"], user_query)

            # 4. Filter by similarity threshold
            filtered_results = self.retriever.filter_by_similarity_threshold(
                boosted_results
            )

            if not filtered_results:
                return self._handle_low_similarity(
                    user_query, retrieval_result["results"]
                )

            # 4. Context preparation
            context_info = self._prepare_context(filtered_results, processed_query)

            # 5. Generate response
            response_data = self._generate_response(
                user_query, context_info, processed_query
            )

            # 6. Evaluate response quality
            evaluation = self._evaluate_response(
                response_data["response"],
                user_query,
                context_info["sources"],
                context_info["documents"],
            )

            # 7. Prepare final result
            # Sadece gerÃ§ekten kullanÄ±lan ilk source'u dÃ¶ndÃ¼r (en yÃ¼ksek skorlu)
            result = {
                "response": response_data["response"],
                "sources": context_info["sources"][:1],  # Ä°lk source (en alakalÄ±)
                "confidence": evaluation["overall_score"],
                "quality_level": evaluation["quality_level"],
                "query_analysis": processed_query,
                "retrieval_info": {
                    "total_found": len(retrieval_result["results"]),
                    "after_filtering": len(filtered_results),
                    "best_score": (
                        filtered_results[0]["combined_score"] if filtered_results else 0
                    ),
                },
                "evaluation": evaluation,
            }

            # 8. Conversation history'ye ekle
            self._add_to_conversation_history(user_query, response_data["response"])

            return result

        except Exception as e:
            logger.error(f"âŒ Query iÅŸleme hatasÄ±: {e}")
            return self._handle_error(user_query, str(e))

    def _expand_query_with_context(self, user_query: str) -> str:
        """Conversation history kullanarak sorguyu geniÅŸlet"""
        
        # KÄ±sa sorgularÄ± veya referans iÃ§eren sorgularÄ± context ile geniÅŸlet
        query_words = user_query.split()
        
        if len(query_words) <= 6 and self.conversation_history:  # 6 kelime veya daha az
            
            # Son soru-cevap Ã§iftini al
            last_qa = self.conversation_history[-1]
            last_question = last_qa[0]
            
            # Referans kelimeler - TÃ¼rkÃ§e'de yaygÄ±n
            reference_words = ['bu', 'ÅŸu', 'o', 'bunun', 'ÅŸunun', 'onun', 'bunu', 'ÅŸunu', 'onu', 
                             'iÃ§in', 'hakkÄ±nda', 'konusunda', 'ile ilgili', 'bahar', 'gÃ¼z', 'yarÄ±yÄ±l',
                             'dÃ¶nemi', 'dÃ¶nem', 'sÄ±nÄ±fÄ±m', 'yapabilir', 'miyim', 'mÄ±yÄ±m', 'mi', 'mÄ±', 
                             'peki', 'ya', 'ayrÄ±ca', 'Ã¶te yandan', 'bir de', 'diÄŸer', 'sonra']
            
            # Mevcut sorgu belirsiz/referans iÃ§erir mi?
            has_reference = any(word in user_query.lower() for word in reference_words)
            is_vague = len(query_words) <= 4
            
            if has_reference or is_vague:
                # Son sorudaki anahtar terimleri Ã§Ä±kar (Ã§ok spesifik olanlarÄ±)
                last_keywords = self._extract_query_keywords(last_question)
                
                # Ã–nemli domain-specific terimleri filtrele
                important_terms = []
                for word in last_keywords:
                    if len(word) > 3 and word not in ['iÃ§in', 'neden', 'nasÄ±l', 'nedir', 'zaman', 'yapmak', 'istiyorum']:
                        important_terms.append(word)
                
                # Son cevaptan da kritik terimleri Ã§Ä±kar
                last_answer = last_qa[1]
                answer_keywords = self._extract_query_keywords(last_answer)
                for word in answer_keywords:
                    if len(word) > 4 and word not in important_terms and word not in ['belge', 'dokuman', 'bilgi', 'konuda', 'iÃ§in']:
                        important_terms.append(word)
                
                # Context relevance check - eÄŸer mevcut soru tamamen farklÄ± ise expand etme
                current_keywords = self._extract_query_keywords(user_query)
                
                # EÄŸer mevcut soruda Ã§ok spesifik ve uzun terimler varsa ve 
                # son soruyla hiÃ§ overlap yoksa expand etme (daha esnek kontrol)
                if len(current_keywords) > 2:  # En az 3 keyword varsa kontrol et
                    # Ã‡ok spesifik current keywords (uzun ve akademik olmayan)
                    very_specific = [w for w in current_keywords if len(w) > 6 and w not in [
                        'prosedÃ¼r', 'belgeleme', 'deÄŸerlendirme', 'yÃ¶nerge', 'yÃ¶nerges',
                        'baÅŸarÄ±', 'iÅŸlem', 'gereklilik', 'muafiyet'
                    ]]
                    
                    if very_specific:  # Ã‡ok spesifik terimler var
                        # Semantic overlap kontrolÃ¼ - daha esnek
                        semantic_overlap = any(
                            term in last_question.lower() or 
                            any(last_word in term for last_word in last_keywords if len(last_word) > 3)
                            for term in very_specific
                        )
                        if not semantic_overlap:
                            # Tamamen farklÄ± konu, expand etme
                            logger.info(f"ğŸš« FarklÄ± konu tespit edildi, expand yapÄ±lmÄ±yor: {very_specific}")
                            return user_query
                
                # En fazla 3 Ã¶nemli terim ekle
                context_terms = important_terms[:3]
                
                if context_terms:
                    expanded = f"{user_query} {' '.join(context_terms)}"
                    logger.info(f"ğŸ”— Query geniÅŸletildi: '{user_query}' â†’ '{expanded}'")
                    return expanded
        
        return user_query

    def _apply_conversation_context_boost(self, results: List[Dict[str, Any]], user_query: str) -> List[Dict[str, Any]]:
        """Conversation history'de kullanÄ±lan belgelere ve konulara ekstra boost ver - GENEL SÄ°STEM"""
        if not self.conversation_history or not results:
            return results
        
        # Vague/belirsiz sorgularÄ± tespit et (conversation context gerektiren)
        # Daha esnek koÅŸullar - sadece kelime sayÄ±sÄ± deÄŸil, conversation pattern'i de Ã¶nemli
        needs_context = False
        
        # KoÅŸul 1: KÄ±sa sorgular (â‰¤10 kelime)
        if len(user_query.split()) <= 10:
            needs_context = True
            
        # KoÅŸul 2: Belirsiz/eksik bilgi iÃ§eren sorgular
        vague_patterns = [
            r'\b(bu|ÅŸu|o)\b.*\b(nedir|nasÄ±l|ne|hangi)\b',  # "bu nedir", "ÅŸu nasÄ±l" 
            r'\b(peki|ya|ayrÄ±ca)\b',                        # "peki", "ya", "ayrÄ±ca"
            r'\b(gÃ¶rev|kurul|sistem|belge)\b.*\bnedir\b',   # "...nedir" ile biten sorular
            r'\b(yapabilir|olabilir|mÃ¼mkÃ¼n)\s+(mi|mÄ±)\b',   # "yapabilir mi" tÃ¼rÃ¼ sorular
        ]
        
        for pattern in vague_patterns:
            if re.search(pattern, user_query.lower()):
                needs_context = True
                break
                
        if needs_context and self.conversation_history:
            
            # Son 2 conversation'dan anahtar terimleri Ã§Ä±kar
            context_keywords = set()
            recent_sources = set()
            
            for question, answer, timestamp in self.conversation_history[-2:]:
                # Sorulardan anahtar kelimeler
                q_keywords = self._extract_query_keywords(question)
                context_keywords.update(q_keywords)
                
                # Cevaplardan da Ã¶nemli terimleri Ã§Ä±kar (fakÃ¼lte adlarÄ±, program isimleri vs.)
                a_keywords = self._extract_context_keywords_from_answer(answer)
                context_keywords.update(a_keywords)
            
            # Son kullanÄ±lan belgeyi tespit et (Ã§ok Ã¶nemli!)
            last_used_sources = set()
            
            # Conversation history'den pattern matching ile kaynak tespit et
            for question, answer, timestamp in self.conversation_history[-2:]:
                # Cevaptan domain pattern'lerini tespit et
                combined_text = (question + " " + answer).lower()
                
                # Domain-specific pattern matching
                if any(term in combined_text for term in ['tercÃ¼me', 'senaryo', 'tÄ±p fakÃ¼ltesi', 'tÄ±p eÄŸitimi']):
                    last_used_sources.add('179492.pdf')  # TÄ±p FakÃ¼ltesi belgesi
                elif any(term in combined_text for term in ['diÅŸ hekimliÄŸi', 'diÅŸ hekimliÄŸi fakÃ¼ltesi']):
                    last_used_sources.add('139037.pdf')  # DiÅŸ hekimliÄŸi belgesi
                elif any(term in combined_text for term in ['yabancÄ± dil', 'dil eÄŸitimi', 'muafiyet']):
                    last_used_sources.add('147419.pdf')  # YabancÄ± dil belgesi
                elif any(term in combined_text for term in ['baÅŸarÄ± deÄŸerlendirme', 'yÃ¼zde on', '%10']):
                    last_used_sources.add('82916.pdf')   # BaÅŸarÄ± deÄŸerlendirme belgesi
                elif any(term in combined_text for term in ['yatay geÃ§iÅŸ', 'transfer', 'nakil']):
                    # Yatay geÃ§iÅŸ birden fazla belgede olabilir, context'e gÃ¶re karar ver
                    if 'diÅŸ hekimliÄŸi' in combined_text:
                        last_used_sources.add('139037.pdf')
                    else:
                        last_used_sources.add('173204.pdf')  # Genel yatay geÃ§iÅŸ belgesi
            
            # Context keywords varsa boost uygula
            if context_keywords:
                logger.info(f"ğŸ”— Conversation context boost uygulanÄ±yor - Context keywords: {list(context_keywords)[:5]}...")
                
                boosted_results = []
                boosted_count = 0
                
                for result in results:
                    source_file = result.get('metadata', {}).get('source_file', '')
                    doc_content = result.get('document', '').lower()
                    
                    # Bu belge conversation context'e ne kadar uygun?
                    relevance_score = self._calculate_context_relevance(doc_content, context_keywords)
                    
                    # EÄŸer bu belge son kullanÄ±lan belgelerden biriyse ekstra boost
                    extra_boost = 1.0
                    for last_source in last_used_sources:
                        if last_source in source_file:
                            extra_boost = 1.8  # %80 ekstra boost
                            logger.info(f"ğŸ¯ Son kullanÄ±lan belge tespit edildi: {source_file} - ekstra boost veriliyor")
                            break
                    
                    if relevance_score > 0.3:  # %30'dan fazla alakalÄ± ise boost ver
                        boosted_result = result.copy()
                        original_score = boosted_result.get('combined_score', 0)
                        # Relevance score'a gÃ¶re deÄŸiÅŸken boost (1.2x - 1.8x) + ekstra boost
                        boost_factor = (1.2 + (relevance_score * 0.6)) * extra_boost
                        boosted_result['combined_score'] = original_score * boost_factor
                        boosted_results.append(boosted_result)
                        boosted_count += 1
                        logger.info(f"ğŸš€ Context boost: {source_file} - {original_score:.3f} â†’ {boosted_result['combined_score']:.3f} (relevance: {relevance_score:.3f}, extra: {extra_boost:.1f}x)")
                    else:
                        boosted_results.append(result)
                
                # Score'a gÃ¶re yeniden sÄ±rala
                boosted_results.sort(key=lambda x: x.get('combined_score', 0), reverse=True)
                
                logger.info(f"ğŸ”— Conversation context boost tamamlandÄ±: {boosted_count} belge boost edildi")
                
                return boosted_results
        
        return results

    def _extract_context_keywords_from_answer(self, answer: str) -> List[str]:
        """Cevaplardan Ã¶nemli context anahtar kelimelerini Ã§Ä±kar"""
        import re
        
        keywords = []
        text_lower = answer.lower()
        
        # FakÃ¼lte/program isimleri
        faculty_patterns = [
            r'(\w+)\s+fakÃ¼ltesi?',
            r'(\w+)\s+bÃ¶lÃ¼mÃ¼',
            r'(\w+)\s+programÄ±',
            r'(\w+)\s+yÃ¼ksekokulu',
            r'(\w+)\s+enstitÃ¼sÃ¼'
        ]
        
        for pattern in faculty_patterns:
            matches = re.findall(pattern, text_lower)
            for match in matches:
                if len(match) > 3:  # Ã‡ok kÄ±sa kelimeler hariÃ§
                    keywords.append(match)
        
        # Ã–nemli genel terimler
        important_terms = [
            'belge', 'baÅŸvuru', 'iÅŸlem', 'prosedÃ¼r', 'kural', 'koÅŸul', 'sÃ¼reÃ§',
            'gerekli', 'ÅŸart', 'onay', 'deÄŸerlendirme', 'baÅŸarÄ±', 'form',
            'yÃ¶netmelik', 'muafiyet', 'nakil', 'transfer', 'sertifika', 'kayÄ±t',
            'dÃ¶nem', 'sÃ¼re', 'gÃ¼ncelleme', 'revizyon', 'kontrol', 'uygunluk'
        ]
        
        for term in important_terms:
            if term in text_lower:
                keywords.append(term)
        
        return list(set(keywords))  # TekrarlarÄ± kaldÄ±r

    def _calculate_context_relevance(self, document: str, context_keywords: set) -> float:
        """Bir belgenin conversation context'e ne kadar alakalÄ± olduÄŸunu hesapla"""
        if not context_keywords:
            return 0.0
        
        doc_lower = document.lower()
        
        # Context keywords'Ã¼n belgede kaÃ§ tanesi var?
        found_keywords = []
        for keyword in context_keywords:
            keyword_lower = keyword.lower()
            if len(keyword_lower) > 2:  # Ã‡ok kÄ±sa kelimeler hariÃ§
                if keyword_lower in doc_lower:
                    found_keywords.append(keyword_lower)
        
        # Relevance score = bulunan keyword oranÄ±
        if len(context_keywords) > 0:
            relevance = len(found_keywords) / len(context_keywords)
        else:
            relevance = 0.0
        
        # Bonus: EÄŸer aynÄ± belgede birden fazla context keyword varsa ek puan
        if len(found_keywords) > 1:
            relevance += 0.2
        
        return min(relevance, 1.0)  # Max 1.0

    def _add_to_conversation_history(self, question: str, answer: str):
        """Conversation history'ye soru-cevap Ã§ifti ekle"""
        import time
        
        # Yeni soru-cevap Ã§iftini ekle
        self.conversation_history.append((question, answer, time.time()))
        
        # History uzunluÄŸunu sÄ±nÄ±rla
        if len(self.conversation_history) > self.max_history_length:
            self.conversation_history = self.conversation_history[-self.max_history_length:]

    def _prepare_context(
        self, results: List[Dict[str, Any]], processed_query: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Retrieval sonuÃ§larÄ±ndan context hazÄ±rla"""

        context_parts = []
        sources = []  # Set yerine list - sÄ±ralamayÄ± koru
        documents = []
        user_query = processed_query.get('original_query', processed_query.get('query', ''))

        for i, result in enumerate(results[: config.DEFAULT_N_RESULTS], 1):
            doc = result["document"]
            metadata = result.get("metadata", {})
            score = result.get("combined_score", 0)

            # Document relevans kontrolÃ¼ - alakasÄ±z dÃ¶kÃ¼manlarÄ± filtrele
            if not self._is_document_relevant_to_query(doc, user_query):
                continue

            # Document preprocessing - user_query ile birlikte
            clean_doc = self._clean_document_for_context(doc, user_query)
            documents.append(clean_doc)

            # Source tracking - sÄ±ralÄ± ve tekrarsÄ±z
            source_file = metadata.get("source_file", f"Belge_{i}")
            if source_file not in sources:  # Duplicate check
                sources.append(source_file)

            # Context formatting
            context_parts.append(
                {
                    "index": i,
                    "content": clean_doc,
                    "source": source_file,
                    "score": score,
                    "chunk_index": metadata.get("chunk_index", 0),
                }
            )

        # Generate rich context
        formatted_context = self._format_context_for_llm(context_parts, processed_query)

        return {
            "formatted_context": formatted_context,
            "sources": sources,  # ArtÄ±k list olarak dÃ¶ndÃ¼r
            "documents": documents,
            "context_parts": context_parts,
        }

    def _clean_document_for_context(self, document: str, user_query: str = "") -> str:
        """DokÃ¼manÄ± context iÃ§in akÄ±llÄ±ca temizle"""
        # Fazla boÅŸluklarÄ± temizle
        clean_doc = " ".join(document.split())
        words = clean_doc.split()
        max_words = config.MAX_CONTEXT_LENGTH // 5  # YaklaÅŸÄ±k kelime baÅŸÄ±na 5 karakter

        # EÄŸer dÃ¶kÃ¼man kÄ±sa ise direkt dÃ¶ndÃ¼r
        if len(words) <= max_words:
            return clean_doc

        # AkÄ±llÄ± kesme: Sorgu anahtar kelimelerini iÃ§eren kÄ±smÄ± bul
        if user_query:
            query_keywords = self._extract_query_keywords(user_query)
            
            # Anahtar kelimelerin geÃ§tiÄŸi yerleri bul
            keyword_positions = []
            doc_lower = clean_doc.lower()
            
            for keyword in query_keywords:
                keyword_lower = keyword.lower()
                start = 0
                while True:
                    pos = doc_lower.find(keyword_lower, start)
                    if pos == -1:
                        break
                    # Kelime pozisyonunu hesapla
                    word_pos = len(doc_lower[:pos].split())
                    keyword_positions.append(word_pos)
                    start = pos + 1
            
            if keyword_positions:
                # En erken anahtar kelime pozisyonunu bul
                earliest_keyword = min(keyword_positions)
                
                # Context'i anahtar kelimeden Ã¶nce ve sonra dengeli daÄŸÄ±t
                context_before = max_words // 3  # 1/3'Ã¼ Ã¶nceki kÄ±sÄ±m
                context_after = max_words - context_before  # 2/3'Ã¼ sonraki kÄ±sÄ±m
                
                start_pos = max(0, earliest_keyword - context_before)
                end_pos = min(len(words), start_pos + max_words)
                
                # EÄŸer son kÄ±sÄ±m kÄ±sa kalÄ±rsa baÅŸtan daha fazla al
                if end_pos - start_pos < max_words:
                    start_pos = max(0, end_pos - max_words)
                
                selected_words = words[start_pos:end_pos]
                prefix = "..." if start_pos > 0 else ""
                suffix = "..." if end_pos < len(words) else ""
                
                return f"{prefix}{' '.join(selected_words)}{suffix}"
        
        # Fallback: Sadece baÅŸtan al
        return " ".join(words[:max_words]) + "..."

    def _is_document_relevant_to_query(self, document: str, user_query: str) -> bool:
        """DÃ¶kÃ¼manÄ±n sorguyla alakalÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        doc_lower = document.lower()
        
        # Genel anahtar kelime kontrolÃ¼
        query_keywords = self._extract_query_keywords(user_query)
        if not query_keywords:
            return True  # Anahtar kelime yoksa tÃ¼m dÃ¶kÃ¼manlarÄ± al
            
        # En az bir anahtar kelime geÃ§meli
        keyword_matches = sum(1 for keyword in query_keywords if keyword in doc_lower)
        
        # EÄŸer Ã§ok uzun dÃ¶kÃ¼man ise (>10000 karakter) daha esnek ol
        if len(document) > 10000:
            return keyword_matches > 0  # En az 1 match yeterli
        else:
            return keyword_matches > 0

    def _format_context_for_llm(
        self, context_parts: List[Dict[str, Any]], processed_query: Dict[str, Any]
    ) -> str:
        """LLM iÃ§in context'i formatla"""

        context_lines = []

        for part in context_parts:
            source_info = f"[{part['source']}]"
            content = part["content"]
            score_info = f"(Uygunluk: {part['score']:.2f})"

            context_lines.append(
                f"{part['index']}. {source_info} {content} {score_info}"
            )

        return "\n\n".join(context_lines)

    def _generate_response(
        self,
        user_query: str,
        context_info: Dict[str, Any],
        processed_query: Dict[str, Any],
    ) -> Dict[str, Any]:
        """GeliÅŸmiÅŸ prompt ile yanÄ±t Ã¼ret"""

        # Query kategorisine gÃ¶re Ã¶zelleÅŸtirilmiÅŸ prompt
        specialized_instructions = self._get_specialized_instructions(
            processed_query["category"]
        )

        # Tek soruya odaklanan prompt oluÅŸtur
        focused_system_prompt = config.SYSTEM_PROMPT + "\n" + specialized_instructions + """

Ã–NEMLI: Sadece kullanÄ±cÄ±nÄ±n ÅŸu anda sorduÄŸu soruya cevap ver. Ã–nceki sorular veya konularla ilgili bilgi verme.
Bu sorguya Ã¶zgÃ¼ ve kesin bir yanÄ±t ver. BaÅŸka konulara deÄŸinme."""

        # Ana prompt oluÅŸtur
        prompt = config.RAG_PROMPT_TEMPLATE.format(
            system_prompt=focused_system_prompt,
            question=user_query,
            context=context_info["formatted_context"],
        )

        # LLM'den yanÄ±t al
        raw_response = ask_local_llm(prompt, model=config.LLM_MODEL)
        clean_response = temizle_yanit(raw_response)

        # YanÄ±t post-processing - tek soruya odaklanarak
        processed_response = self._post_process_response(
            clean_response, context_info["sources"], user_query
        )

        return {
            "response": processed_response,
            "raw_response": raw_response,
            "prompt_used": prompt,
        }

    def _get_specialized_instructions(self, query_category: str) -> str:
        """Query kategorisine gÃ¶re Ã¶zel talimatlar"""
        instructions = {
            "procedure": "\nProsedÃ¼r sorularÄ±nda adÄ±m adÄ±m aÃ§Ä±klama yap. SÄ±ralÄ± iÅŸlemler ver.",
            "temporal": "\nTarih ve zaman bilgilerini kesin olarak belirt. 'yaklaÅŸÄ±k' gibi belirsiz ifadeler kullanma.",
            "quantitative": "\nSayÄ±sal bilgileri tam olarak ver. Belirsizlik varsa bunu aÃ§Ä±kÃ§a belirt.",
            "definition": "\nTanÄ±mlarÄ± net ve anlaÅŸÄ±lÄ±r ÅŸekilde yap. Ã–rnekler ver.",
            "explanation": "\nSebep-sonuÃ§ iliÅŸkilerini aÃ§Ä±kla. MantÄ±klÄ± gerekÃ§eler sun.",
            "location": "\nYer bilgilerini spesifik olarak belirt.",
            "general": "\nKapsamlÄ± ve dÃ¼zenli bir aÃ§Ä±klama yap.",
        }

        return instructions.get(query_category, instructions["general"])

    def _post_process_response(self, response: str, sources: List[str], user_query: str = None) -> str:
        """YanÄ±tÄ± son iÅŸleme tabi tut"""

        # Kaynak bilgilerini temizle - LLM'den gelen kaynak referanslarÄ±nÄ± kaldÄ±r
        import re
        
        # EÄŸer bu bir fallback response ise, sadece temel temizlik yap
        if response == config.FALLBACK_RESPONSE or "belgelerimde yeterli bilgi bulunmuyor" in response:
            # Sadece temel temizlik
            response = re.sub(r'\s+', ' ', response)
            return response.strip()
        
        # Normal cevaplar iÃ§in sadece zararlÄ± dosya referanslarÄ±nÄ± temizle
        # Sadece cÃ¼mle sonundaki dosya adlarÄ±nÄ± ve aÃ§Ä±k kaynak referanslarÄ±nÄ± temizle
        
        # 1. Kaynak referanslarÄ±nÄ± kapsamlÄ± temizleme
        response = re.sub(r'\s*Kaynak:\s*\[.*?\].*?$', '', response, flags=re.IGNORECASE | re.MULTILINE)
        response = re.sub(r'\s*Kaynak:\s*.*?\.pdf.*?$', '', response, flags=re.IGNORECASE | re.MULTILINE)
        response = re.sub(r'\s*Kaynak:\s*.*?\.docx.*?$', '', response, flags=re.IGNORECASE | re.MULTILINE)
        response = re.sub(r'\s*Kaynak belge:\s*.*?$', '', response, flags=re.IGNORECASE | re.MULTILINE)
        response = re.sub(r'\s*Kaynaklar:\s*.*?$', '', response, flags=re.IGNORECASE | re.MULTILINE)
        
        # 2. KÃ¶ÅŸeli parantezlerle Ã§evrili dosya referanslarÄ±
        response = re.sub(r'\s*\[.*?\.pdf\].*?$', '', response, flags=re.IGNORECASE | re.MULTILINE)
        response = re.sub(r'\s*\[.*?\.docx\].*?$', '', response, flags=re.IGNORECASE | re.MULTILINE)
        
        # 3. Ã–zel dosya adÄ± formatlarÄ±nÄ± temizle
        response = re.sub(r'\s*\d+-\d+-\d+_[A-Za-z0-9_-]+\.pdf\s*', ' ', response, flags=re.IGNORECASE)
        response = re.sub(r'\s*[A-Z]+\.\d+\.[A-Z]+\.\d+.*?\.pdf\s*', ' ', response, flags=re.IGNORECASE)
        
        # 4. Eksik parantezleri ve noktalama iÅŸaretlerini temizle
        response = re.sub(r'\s*\($\s*', '', response, flags=re.MULTILINE)
        response = re.sub(r'\s*\)$\s*', '', response, flags=re.MULTILINE)
        response = re.sub(r'\s*\[\s*$', '', response, flags=re.MULTILINE)
        response = re.sub(r'\s*\]\s*$', '', response, flags=re.MULTILINE)
        response = re.sub(r'\s*[,;:]\s*$', '', response, flags=re.MULTILINE)
        
        # 5. Ã‡oklu boÅŸluklarÄ± ve satÄ±r sonlarÄ±nÄ± dÃ¼zelt
        response = re.sub(r'\s+', ' ', response)
        response = re.sub(r'\s*\n\s*', '\n', response)
        
        # 6. Tekrar eden nokta iÅŸaretlerini dÃ¼zelt
        response = re.sub(r'\.{2,}', '.', response)
        
        # 7. CÃ¼mle bitiÅŸlerini dÃ¼zelt
        response = re.sub(r'\s*\.\s*', '. ', response)
        response = response.strip()
        
        # 8. Ã‡ok kÄ±sa yanÄ±tlarÄ± geniÅŸlet
        if len(response) < config.MIN_ANSWER_LENGTH:
            response += " Bu konuda daha detaylÄ± bilgi iÃ§in ilgili belgeleri inceleyebilirsiniz."

        return response.strip()

    def _filter_response_for_single_query(self, response: str, user_query: str) -> str:
        """YanÄ±tÄ± tek soruya odaklayacak ÅŸekilde filtrele"""
        
        # Ã–nce alakasÄ±z ifadeleri temizle
        response = self._remove_irrelevant_phrases(response, user_query)
        
        # EÄŸer yanÄ±t Ã§ok kÄ±sa kaldÄ±ysa, LLM'den gelen orijinal yanÄ±tÄ± kullan
        if len(response.strip()) < 50:
            return response
        
        # KullanÄ±cÄ±nÄ±n sorusundaki anahtar kelimeleri Ã§Ä±kar
        query_keywords = self._extract_query_keywords(user_query)
        
        # YanÄ±tÄ± cÃ¼mlelere bÃ¶l (nokta, Ã¼nlem, soru iÅŸaretiyle)
        sentences = re.split(r'[.!?]+', response)
        sentences = [s.strip() for s in sentences if s.strip()]  # BoÅŸ cÃ¼mleleri temizle
        
        filtered_sentences = []
        
        # Ä°lk iki cÃ¼mleyi her zaman dahil et (genellikle doÄŸru cevap)
        if sentences:
            filtered_sentences.extend(sentences[:2])
        
        # DiÄŸer cÃ¼mleleri relevans kontrolÃ¼nden geÃ§ir
        for sentence in sentences[2:]:
            if self._is_sentence_relevant_to_query(sentence, query_keywords, user_query):
                filtered_sentences.append(sentence)
            else:
                # Ä°lgisiz cÃ¼mle bulunduÄŸunda dur (Ã§oklu konu yanÄ±tÄ±nÄ± Ã¶nle)
                break
        
        # CÃ¼mleleri doÄŸru ÅŸekilde birleÅŸtir
        if filtered_sentences:
            filtered_response = '. '.join(filtered_sentences)
            if not filtered_response.endswith('.'):
                filtered_response += '.'
        else:
            filtered_response = response  # Fallback
        
        return filtered_response.strip()

    def _extract_query_keywords(self, query: str) -> list:
        """Sorgudan anahtar kelimeleri Ã§Ä±kar"""
        # TÃ¼rkÃ§e stop words
        stop_words = {'ve', 'ile', 'iÃ§in', 'de', 'da', 'bir', 'bu', 'ÅŸu', 'o', 'ben', 'sen', 'biz', 'siz', 'onlar',
                     'nasÄ±l', 'ne', 'nedir', 'kim', 'nerede', 'neden', 'niÃ§in', 'hangi', 'kaÃ§', 'ne zaman'}
        
        words = query.lower().split()
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        return keywords

    def _is_sentence_relevant_to_query(self, sentence: str, query_keywords: list, user_query: str) -> bool:
        """CÃ¼mlenin sorguyla alakalÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        sentence_lower = sentence.lower()
        
        # Anahtar kelime match kontrolÃ¼
        keyword_matches = sum(1 for keyword in query_keywords if keyword in sentence_lower)
        
        # EÄŸer hiÃ§ anahtar kelime yoksa alakasÄ±z
        return keyword_matches > 0

    def _remove_irrelevant_phrases(self, response: str, user_query: str) -> str:
        """AlakasÄ±z ifadeleri temizle"""
        
        # Genel alakasÄ±z baÅŸlangÄ±Ã§larÄ± temizle
        irrelevant_starts = [
            r'^[^.]*belgede[^.]*kurallar[^.]*\.',
            r'^[^.]*ancak[^.]*\.',
            r'^[^.]*eÄŸer[^.]*\.'
        ]
        
        for pattern in irrelevant_starts:
            response = re.sub(pattern, '', response, flags=re.IGNORECASE)
        
        # Ã‡oklu boÅŸluklarÄ± ve nokta hatalarÄ±nÄ± dÃ¼zelt
        response = re.sub(r'\s+', ' ', response)
        response = re.sub(r'\.+', '.', response)
        response = re.sub(r'\s*\.\s*', '. ', response)
        
        # BaÅŸÄ±nda/sonunda gereksiz boÅŸluk ve nokta temizle
        response = response.strip(' .')
        
        # EÄŸer cÃ¼mle noktayla bitmiyorsa ekle
        if response and not response.endswith('.'):
            response += '.'
            
        return response

    def _evaluate_response(
        self, response: str, query: str, sources: List[str], documents: List[str]
    ) -> Dict[str, Any]:
        """YanÄ±t kalitesini deÄŸerlendir"""
        return self.evaluator.evaluate_response(response, query, sources, documents)

    def _handle_no_results(self, query: str) -> Dict[str, Any]:
        """SonuÃ§ bulunamadÄ±ÄŸÄ±nda"""
        return {
            "response": config.FALLBACK_RESPONSE,
            "sources": [],
            "confidence": 0.0,
            "quality_level": "Bilgi Yok",
            "query_analysis": self.query_processor.process_query(query),
            "retrieval_info": {"total_found": 0, "after_filtering": 0, "best_score": 0},
            "evaluation": {
                "overall_score": 0.0,
                "improvement_suggestions": ["Daha spesifik soru sorun"],
            },
        }

    def _handle_low_similarity(
        self, query: str, results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """DÃ¼ÅŸÃ¼k benzerlik skorlarÄ±nda"""
        best_score = results[0]["combined_score"] if results else 0

        response = config.FALLBACK_RESPONSE

        return {
            "response": response,
            "sources": [],
            "confidence": best_score,
            "quality_level": "DÃ¼ÅŸÃ¼k GÃ¼ven",
            "query_analysis": self.query_processor.process_query(query),
            "retrieval_info": {
                "total_found": len(results),
                "after_filtering": 0,
                "best_score": best_score,
            },
            "evaluation": {
                "overall_score": 0.2,
                "improvement_suggestions": ["Sorguyu yeniden formÃ¼le edin"],
            },
        }

    def _handle_error(self, query: str, error_msg: str) -> Dict[str, Any]:
        """Hata durumunda"""
        return {
            "response": f"ÃœzgÃ¼nÃ¼m, sorunuzu iÅŸlerken bir hata oluÅŸtu: {error_msg}",
            "sources": [],
            "confidence": 0.0,
            "quality_level": "Hata",
            "error": error_msg,
            "evaluation": {"overall_score": 0.0},
        }

    def get_conversation_summary(self) -> Dict[str, Any]:
        """Conversation summary - now delegated to external conversation manager"""
        return {"total_queries": 0, "avg_confidence": 0.0, "recent_queries": []}


def run_interactive_chatbot():
    """Interaktif chatbot'u Ã§alÄ±ÅŸtÄ±r"""
    chatbot = AdvancedRAGChatbot()

    print("\nğŸ¤– GeliÅŸmiÅŸ RAG Chatbot baÅŸlatÄ±ldÄ±!")
    print("ğŸ’¡ Komutlar: 'exit' (Ã§Ä±kÄ±ÅŸ), 'history' (geÃ§miÅŸ), 'help' (yardÄ±m)\n")

    while True:
        try:
            user_input = input("KullanÄ±cÄ±: ").strip()

            if user_input.lower() in ["exit", "quit", "bye"]:
                summary = chatbot.get_conversation_summary()
                print(
                    f"\nBot: GÃ¶rÃ¼ÅŸmek Ã¼zere! Toplam {summary['total_queries']} soru sordunuz."
                )
                print(f"Ortalama gÃ¼ven skoru: {summary['avg_confidence']:.2f}")
                break

            elif user_input.lower() == "history":
                summary = chatbot.get_conversation_summary()
                print(f"\nğŸ“Š KonuÅŸma Ã–zeti:")
                print(f"Toplam soru: {summary['total_queries']}")
                print(f"Ortalama gÃ¼ven: {summary['avg_confidence']:.2f}")
                if summary["recent_queries"]:
                    print("Son sorular:", summary["recent_queries"])
                continue

            elif user_input.lower() == "help":
                print("\nğŸ”§ YardÄ±m:")
                print("- Normal sorularÄ±nÄ±zÄ± yazabilirsiniz")
                print("- 'history' komutu konuÅŸma geÃ§miÅŸini gÃ¶sterir")
                print("- 'exit' komutu chatbot'tan Ã§Ä±kar")
                print("- Daha kesin cevaplar iÃ§in spesifik sorular sorun")
                continue

            if not user_input:
                continue

            # Process query
            result = chatbot.process_query(user_input)

            # Display response
            print(f"\nBot: {result['response']}")

            # Display quality info (opsiyonel)
            print(
                f"\nğŸ“Š GÃ¼ven: {result['confidence']:.2f} | Kalite: {result['quality_level']}"
            )
            if result.get("sources"):
                print(f"ğŸ“š Kaynaklar: {', '.join(result['sources'][:3])}")

            print()  # BoÅŸ satÄ±r

        except KeyboardInterrupt:
            print("\nBot: GÃ¶rÃ¼ÅŸmek Ã¼zere!")
            break
        except Exception as e:
            print(f"âš ï¸ Hata oluÅŸtu: {e}")
            continue


if __name__ == "__main__":
    run_interactive_chatbot()


def get_answer(question: str) -> str:
    """Basit API fonksiyonu - batch_ask.py iÃ§in"""
    try:
        chatbot = AdvancedRAGChatbot()
        result = chatbot.process_query(question)
        
        # Kaynak bilgisini ekle
        response = result['response']
        if result.get('sources'):
            sources = ', '.join(result['sources'][:2])  # Ä°lk 2 kaynaÄŸÄ± al
            response += f"\n\nKullanÄ±lan kaynak: {sources}"
        
        return response
    except Exception as e:
        return f"Hata oluÅŸtu: {str(e)}"